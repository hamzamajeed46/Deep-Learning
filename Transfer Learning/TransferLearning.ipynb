{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxtY0QCvD2dX",
        "outputId": "03510217-4c57-4c77-f8a0-691aeb270e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/karakaggle/kaggle-cat-vs-dog-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 787M/787M [00:07<00:00, 104MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/karakaggle/kaggle-cat-vs-dog-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"karakaggle/kaggle-cat-vs-dog-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2II-p_2fSequ",
        "outputId": "589e6523-d234-416c-bb3a-969a1df03476"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:935: UserWarning: Truncated File Read\n",
            "  warnings.warn(str(msg))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset cleaned and split into train/val/test subsets.\n",
            "Found 17471 images belonging to 2 classes.\n",
            "Found 4990 images belonging to 2 classes.\n",
            "Found 2498 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from PIL import UnidentifiedImageError\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 5\n",
        "TRAIN_RATIO = 0.7\n",
        "VAL_RATIO = 0.2\n",
        "\n",
        "DATASET_PATH = os.path.join(path, 'kagglecatsanddogs_3367a', 'PetImages')\n",
        "SPLIT_DATASET_PATH = \"PetImages_Split\"\n",
        "CLASSES = [\"Cat\", \"Dog\"]\n",
        "\n",
        "# Function to clean and split the dataset\n",
        "def clean_and_split_dataset(dataset_path, output_path, train_ratio=0.7, val_ratio=0.2):\n",
        "    # Create train/val/test directories\n",
        "    for subset in [\"train\", \"val\", \"test\"]:\n",
        "        for cls in CLASSES:\n",
        "            os.makedirs(os.path.join(output_path, subset, cls), exist_ok=True)\n",
        "\n",
        "    # Process and split each class\n",
        "    for cls in CLASSES:\n",
        "        cls_path = os.path.join(dataset_path, cls)\n",
        "        images = [f for f in os.listdir(cls_path) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "\n",
        "        # Check and clean corrupt images\n",
        "        valid_images = []\n",
        "        for img in images:\n",
        "            img_path = os.path.join(cls_path, img)\n",
        "            try:\n",
        "                tf.keras.preprocessing.image.load_img(img_path).close()\n",
        "                valid_images.append(img)\n",
        "            except (UnidentifiedImageError, OSError):\n",
        "                os.remove(img_path)\n",
        "                print(f\"Removed corrupt image: {img_path}\")\n",
        "\n",
        "        # Split into train, val, and test\n",
        "        train, temp = train_test_split(valid_images, train_size=train_ratio, random_state=42)\n",
        "        val, test = train_test_split(temp, test_size=(1 - train_ratio - val_ratio) / (1 - train_ratio), random_state=42)\n",
        "\n",
        "        # Move files into respective folders\n",
        "        for subset, split in zip([\"train\", \"val\", \"test\"], [train, val, test]):\n",
        "            for img in split:\n",
        "                src = os.path.join(cls_path, img)\n",
        "                dst = os.path.join(output_path, subset, cls, img)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "    print(\"Dataset cleaned and split into train/val/test subsets.\")\n",
        "\n",
        "# Clean and split the dataset\n",
        "clean_and_split_dataset(DATASET_PATH, SPLIT_DATASET_PATH)\n",
        "\n",
        "# Data generators\n",
        "data_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Training and validation datasets\n",
        "train_generator = data_gen.flow_from_directory(\n",
        "    os.path.join(SPLIT_DATASET_PATH, \"train\"),\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "validation_generator = data_gen.flow_from_directory(\n",
        "    os.path.join(SPLIT_DATASET_PATH, \"val\"),\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\"\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_generator = data_gen.flow_from_directory(\n",
        "    os.path.join(SPLIT_DATASET_PATH, \"test\"),\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"binary\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WmsXlnjhNtT"
      },
      "source": [
        "**Training Base Model (MobileNet) on \"Cat vs Dog\" dataset and checking results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooj9YFKyEtUH",
        "outputId": "aa9ed1c1-ccd6-4fc4-d315-fa3eaf6cb1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retraining the MobileNet model from scratch...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 791ms/step - accuracy: 0.5734 - loss: 0.7367 - val_accuracy: 0.4996 - val_loss: 0.7699\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 563ms/step - accuracy: 0.6695 - loss: 0.6139 - val_accuracy: 0.4996 - val_loss: 0.8903\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 507ms/step - accuracy: 0.7422 - loss: 0.5208 - val_accuracy: 0.4996 - val_loss: 1.0570\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 510ms/step - accuracy: 0.7965 - loss: 0.4385 - val_accuracy: 0.5351 - val_loss: 0.6811\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 513ms/step - accuracy: 0.8372 - loss: 0.3678 - val_accuracy: 0.7677 - val_loss: 0.4839\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 704ms/step - accuracy: 0.7687 - loss: 0.4815\n",
            "Validation Loss: 0.4835\n",
            "Validation Accuracy: 0.7666\n"
          ]
        }
      ],
      "source": [
        "# Function to create MobileNet model for retraining\n",
        "def create_model():\n",
        "    # Initialize MobileNet with random weights\n",
        "    base_mobilenet = tf.keras.applications.MobileNet(\n",
        "        input_shape=(*IMAGE_SIZE, 3),\n",
        "        include_top=False,\n",
        "        # No pre-trained weights\n",
        "        weights=None\n",
        "    )\n",
        "    model = Sequential([\n",
        "        base_mobilenet,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "        # Binary classification output\n",
        "        Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and train model\n",
        "model = create_model()\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"Retraining the MobileNet model from scratch...\")\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=EPOCHS\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "loss, accuracy = model.evaluate(test_generator)\n",
        "print(f\"Validation Loss: {loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd2VF3Gfhbjx"
      },
      "source": [
        "**Checking results after performing Transfer Learning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tVIJkSdHiRw",
        "outputId": "a9a2ce9b-6316-47d9-c549-50fe63fd639f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training Transfer Learning Model (MobileNet with ImageNet weights)...\n",
            "Epoch 1/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 449ms/step - accuracy: 0.8316 - loss: 0.3592 - val_accuracy: 0.9808 - val_loss: 0.0619\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 476ms/step - accuracy: 0.9660 - loss: 0.0902 - val_accuracy: 0.9838 - val_loss: 0.0472\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 415ms/step - accuracy: 0.9727 - loss: 0.0748 - val_accuracy: 0.9862 - val_loss: 0.0413\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 468ms/step - accuracy: 0.9767 - loss: 0.0638 - val_accuracy: 0.9846 - val_loss: 0.0406\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 416ms/step - accuracy: 0.9777 - loss: 0.0581 - val_accuracy: 0.9864 - val_loss: 0.0362\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 331ms/step - accuracy: 0.9846 - loss: 0.0361\n",
            "Test Loss: 0.0367\n",
            "Test Accuracy: 0.9856\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "\n",
        "def create_transfer_learning_model():\n",
        "    # Load the MobileNet base model\n",
        "    base_mobilenet = MobileNet(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "    # Freeze the base layers\n",
        "    base_mobilenet.trainable = False\n",
        "\n",
        "    # Create a new model by adding custom layers on top of the pre-trained base\n",
        "    model = Sequential([\n",
        "        base_mobilenet,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dropout(0.5),\n",
        "        # Output layer for binary classification\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile and train transfer learning model\n",
        "transfer_learning_model = create_transfer_learning_model()\n",
        "transfer_learning_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "print(\"Training Transfer Learning Model (MobileNet with ImageNet weights)...\")\n",
        "history_transfer = transfer_learning_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "\n",
        "transfer_learning_model.save('transfer_learning_mobilenet_model.keras')\n",
        "\n",
        "#MODEL EVALUATION\n",
        "\n",
        "transfer_learning_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "loss, accuracy = transfer_learning_model.evaluate(test_generator)\n",
        "\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXXqPHvAZuE9"
      },
      "outputs": [],
      "source": [
        "# Use the following base models to evaluate the performance on the \"Cat vs Dog\" dataset\n",
        "# LeNet\n",
        "# AlexNet\n",
        "# VGGNet\n",
        "# ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz7KqrPPieF-",
        "outputId": "a283439a-a247-49ee-92e5-18dd96248c07"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 492ms/step - accuracy: 0.5671 - loss: 0.7543 - val_accuracy: 0.6281 - val_loss: 0.6451\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 405ms/step - accuracy: 0.6515 - loss: 0.6237 - val_accuracy: 0.6349 - val_loss: 0.6343\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 413ms/step - accuracy: 0.6550 - loss: 0.6138 - val_accuracy: 0.6553 - val_loss: 0.6100\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 404ms/step - accuracy: 0.6816 - loss: 0.5828 - val_accuracy: 0.6625 - val_loss: 0.6156\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 476ms/step - accuracy: 0.7087 - loss: 0.5634 - val_accuracy: 0.6908 - val_loss: 0.5772\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6a3f7933a0>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "def create_lenet_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(6, kernel_size=(5,5), activation='tanh', input_shape=(*IMAGE_SIZE, 3), padding='same'),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        Conv2D(16, kernel_size=(5,5), activation='tanh'),\n",
        "        MaxPooling2D(pool_size=(2,2)),\n",
        "        Flatten(),\n",
        "        Dense(120, activation='tanh'),\n",
        "        Dense(84, activation='tanh'),\n",
        "        Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "lenet_model = create_lenet_model()\n",
        "lenet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lenet_model.fit(train_generator, validation_data=validation_generator, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "px9_rWbvidHv",
        "outputId": "84d15bbf-9030-4905-8cc8-b6119f9837bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 579ms/step - accuracy: 0.4974 - loss: 0.8372 - val_accuracy: 0.5004 - val_loss: 0.6931\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 424ms/step - accuracy: 0.4927 - loss: 0.6933 - val_accuracy: 0.4996 - val_loss: 0.6932\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 423ms/step - accuracy: 0.4969 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6931\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 425ms/step - accuracy: 0.5103 - loss: 0.6932 - val_accuracy: 0.5004 - val_loss: 0.6932\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 420ms/step - accuracy: 0.4920 - loss: 0.6934 - val_accuracy: 0.5004 - val_loss: 0.6931\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6a3ed51720>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_alexnet_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(96, kernel_size=(11,11), strides=4, activation='relu', input_shape=(*IMAGE_SIZE, 3)),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=2),\n",
        "        Conv2D(256, kernel_size=(5,5), padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=2),\n",
        "        Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'),\n",
        "        Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'),\n",
        "        Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'),\n",
        "        MaxPooling2D(pool_size=(3,3), strides=2),\n",
        "        Flatten(),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(4096, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')  # Binary classification\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "alexnet_model = create_alexnet_model()\n",
        "alexnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "alexnet_model.fit(train_generator, validation_data=validation_generator, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DltkBR5oidEU",
        "outputId": "217055d4-d9eb-40fc-ed72-29231741a7d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 1s/step - accuracy: 0.8102 - loss: 0.3998 - val_accuracy: 0.9477 - val_loss: 0.1425\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 786ms/step - accuracy: 0.9423 - loss: 0.1451 - val_accuracy: 0.9593 - val_loss: 0.1124\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 781ms/step - accuracy: 0.9554 - loss: 0.1139 - val_accuracy: 0.9525 - val_loss: 0.1210\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 896ms/step - accuracy: 0.9551 - loss: 0.1128 - val_accuracy: 0.9607 - val_loss: 0.1034\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 774ms/step - accuracy: 0.9579 - loss: 0.1064 - val_accuracy: 0.9617 - val_loss: 0.0995\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6a47263ca0>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "def create_vgg_model():\n",
        "    base_vgg = VGG16(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "    base_vgg.trainable = False\n",
        "    model = Sequential([\n",
        "        base_vgg,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "vgg_model = create_vgg_model()\n",
        "vgg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "vgg_model.fit(train_generator, validation_data=validation_generator, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riRb74LbidBu",
        "outputId": "6e5a6dd9-893d-43b0-c2e6-56d482459fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 710ms/step - accuracy: 0.5892 - loss: 0.7007 - val_accuracy: 0.6928 - val_loss: 0.6038\n",
            "Epoch 2/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 508ms/step - accuracy: 0.6537 - loss: 0.6191 - val_accuracy: 0.7002 - val_loss: 0.5806\n",
            "Epoch 3/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 518ms/step - accuracy: 0.6695 - loss: 0.6035 - val_accuracy: 0.6886 - val_loss: 0.5887\n",
            "Epoch 4/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 548ms/step - accuracy: 0.6964 - loss: 0.5860 - val_accuracy: 0.7040 - val_loss: 0.5700\n",
            "Epoch 5/5\n",
            "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 550ms/step - accuracy: 0.6834 - loss: 0.5854 - val_accuracy: 0.7291 - val_loss: 0.5553\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c6a469891e0>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "def create_resnet_model():\n",
        "    base_resnet = ResNet50(input_shape=(*IMAGE_SIZE, 3), include_top=False, weights='imagenet')\n",
        "    base_resnet.trainable = False\n",
        "    model = Sequential([\n",
        "        base_resnet,\n",
        "        GlobalAveragePooling2D(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "resnet_model = create_resnet_model()\n",
        "resnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "resnet_model.fit(train_generator, validation_data=validation_generator, epochs=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nSns8i64iFup"
      },
      "outputs": [],
      "source": [
        "# Perform Transfer Learning using the following models and check results\n",
        "# LeNet\n",
        "# AlexNet\n",
        "# VGGNet\n",
        "# ResNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "thDLE0bfjR6V",
        "outputId": "21e1c774-2895-49fb-e478-d0e4a8526520"
      },
      "outputs": [
        {
          "ename": "ResourceExhaustedError",
          "evalue": "{{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[802816,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e54406509587>\u001b[0m in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mlenet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlenet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0malexnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malexnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mvggnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mresnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e54406509587>\u001b[0m in \u001b[0;36mvgg_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Additional layers as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/random.py\u001b[0m in \u001b[0;36muniform\u001b[0;34m(shape, minval, maxval, dtype, seed)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cast_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdraw_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     return tf.random.stateless_uniform(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mminval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: {{function_node __wrapped__StatelessRandomUniformV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[802816,4096] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:StatelessRandomUniformV2] name: "
          ]
        }
      ],
      "source": [
        "# Train and Evaluate Models\n",
        "\n",
        "def lenet_model(input_shape=(32, 32, 3)):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.Conv2D(6, (5, 5), activation='relu')(inputs)\n",
        "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)  # Specify pool_size\n",
        "    x = tf.keras.layers.Conv2D(16, (5, 5), activation='relu')(x)\n",
        "    x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)  # Specify pool_size\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(120, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(84, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# Ensure this is how your vgg_model works\n",
        "def vgg_model(input_shape=(224, 224, 3)):\n",
        "    inputs = tf.keras.Input(shape=input_shape)  # Create the input tensor\n",
        "    # Add the VGG layers here\n",
        "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    # Additional layers as needed\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dense(4096, activation='relu')(x)\n",
        "    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "\n",
        "def train_and_evaluate(model, name):\n",
        "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    print(f\"Training {name}...\")\n",
        "    history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, verbose=1)\n",
        "    loss, acc = model.evaluate(test_generator, verbose=1)\n",
        "    print(f\"{name} Test Loss: {loss:.4f}\")\n",
        "    print(f\"{name} Test Accuracy: {acc:.4f}\\n\")\n",
        "    return acc\n",
        "\n",
        "# Instantiate Models with Correct Input Sizes\n",
        "lenet = lenet_model(input_shape=(32, 32, 3))\n",
        "alexnet = alexnet_model(input_shape=(227, 227, 3))\n",
        "vggnet = vgg_model(input_shape=(224, 224, 3))\n",
        "resnet = resnet_model(input_shape=(224, 224, 3))\n",
        "\n",
        "# Train and Evaluate\n",
        "results = {\n",
        "    \"LeNet\": train_and_evaluate(lenet, \"LeNet\"),\n",
        "    \"AlexNet\": train_and_evaluate(alexnet, \"AlexNet\"),\n",
        "    \"VGGNet\": train_and_evaluate(vggnet, \"VGGNet\"),\n",
        "    \"ResNet\": train_and_evaluate(resnet, \"ResNet\"),\n",
        "}\n",
        "\n",
        "# Print Final Results\n",
        "print(\"Final Model Performance:\")\n",
        "for model_name, acc in results.items():\n",
        "    print(f\"{model_name}: {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
